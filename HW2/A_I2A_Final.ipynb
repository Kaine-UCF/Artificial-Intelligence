{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A.I2A Final",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kaine-UCF/Artificial-Intelligence/blob/master/HW2/A_I2A_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-_If1qQ6ci-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "6f892cd7-6d37-4066-b47e-4fe94562231c"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "\n",
        "'''\n",
        "Main Function. Initilizes the feature vector. Reshapes training and testing. Stacks the feature vector with input vector.\n",
        "'''\n",
        "def Main():\n",
        "  Features = np.array([[0]*3]*60000) #matrix to hold the values of the custom features\n",
        "  Testing = np.array([[0]*3]*10000) #matrix to hold the values of the custom features\n",
        "\n",
        "  (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "  train_images = train_images.reshape(60000, 28*28) #reshape train images\n",
        "  test_images = test_images.reshape(10000, 28*28) #reshape test images\n",
        "  test_images = np.concatenate((test_images,Testing), axis = 1) # make the testing matrix the same size as training (to prevent error: expected dense_23_input to have shape (787,) but got array with shape (784,) ) \n",
        "\n",
        "  train_images = train_images / 255.0 #normalize between 0-1\n",
        "  test_images = test_images / 255.0 # normalize between 0-1\n",
        " \n",
        "  for X in range(60000):\n",
        "    Features[X][0] = calcHeight(train_images[X]) # calculate height of current image, and place it inside the appropriate index of the features matrix\n",
        "    Features[X][1] = calcWidth(train_images[X]) # calculate width of current image, and place it inside the appropriate index of the features matrix\n",
        "    #Features[X][2] = calcWhiteSpaces(train_images[X]) \n",
        "\n",
        "  train_images = np.concatenate((train_images,Features), axis = 1) #stack our feature vector onto our training vector \n",
        "  MachineLearning(train_images, train_labels, test_images, test_labels)\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Function that contains all the machine learning components \n",
        "'''\n",
        "def MachineLearning(train_images, train_labels, test_images, test_labels):\n",
        "  baseModel = keras.Sequential(keras.layers.Dense(10, activation=tf.nn.softmax))   # set up the softmax activation layer\n",
        "  baseModel.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])  # compile the model \n",
        "\n",
        "  # train the model\n",
        "  epochs = 4\n",
        "  history = baseModel.fit(train_images, train_labels, epochs=epochs, validation_data=(test_images, test_labels))\n",
        "  test_loss, test_acc = baseModel.evaluate(test_images, test_labels)\n",
        "\n",
        "  print('Test accuracy:', test_acc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Function to calulate the normalized height of number contained within the passed image \n",
        "'''\n",
        "def calcHeight(input_image): # The image after being flattened into a 1d vector\n",
        "\n",
        "  topIndex = np.nonzero(input_image) # get indices of all non-zero elements\n",
        "  topIndex = topIndex[0][0] # get the first one\n",
        "\n",
        "  reversedImage = input_image[::-1] # reverse the image array \n",
        " \n",
        "  bottomIndex = np.nonzero(reversedImage) #get indices of all non-zero elements of the reversed array \n",
        "  bottomIndex = 783 - bottomIndex[0][0] # get the first one\n",
        "  return (bottomIndex - topIndex)/784 # return the normalized value \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Function to calulate the normalized width of number contained within the passed image \n",
        "'''\n",
        "def calcWidth(input_image):\n",
        "  reversedImage = input_image[::-1] # reverse the image array \n",
        "  break1 = False\n",
        "  break2 = False\n",
        "# for finding the lefmost index\n",
        "  for i in range(0,28): # sudo rows \n",
        "    for X in range(0,784): # all pixels \n",
        "      if (X % 28 == i): # only check the pixels in the sudo row \n",
        "        if (input_image[X] != 0):\n",
        "          leftIndex = X\n",
        "          break1 = True\n",
        "          break\n",
        "      if(break1 == True):\n",
        "        break\n",
        "# for finding the rightmost index\n",
        "    for X in range(783,0,-1):\n",
        "      if (X % 28 == i):\n",
        "        if (reversedImage[X] != 0):\n",
        "          rightIndex = X\n",
        "          break2 = True\n",
        "          break\n",
        "      if(break2 == True):\n",
        "        break\n",
        "  width = 28 - ((leftIndex%28) + (28-(784-rightIndex)%28))\n",
        "  return width\n",
        "\n",
        "'''\n",
        "The base model had an accuracy of ~92%, while this model has an accuracy of ~87%.\n",
        "The drop in accuracy means that the chosen features were not good ones. \n",
        "I beleive this is because many of the digits had very simliar width and height values. This did not give the \n",
        "system any more useful information to work with, and increased the amount of useless information it had to sort through.\n",
        "'''\n",
        "Main()\n",
        "\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/4\n",
            "60000/60000 [==============================] - 6s 99us/sample - loss: 0.4635 - acc: 0.8771 - val_loss: 0.4540 - val_acc: 0.8561\n",
            "Epoch 2/4\n",
            "60000/60000 [==============================] - 6s 95us/sample - loss: 0.2959 - acc: 0.9172 - val_loss: 0.4856 - val_acc: 0.8440\n",
            "Epoch 3/4\n",
            "60000/60000 [==============================] - 6s 95us/sample - loss: 0.2763 - acc: 0.9220 - val_loss: 0.4469 - val_acc: 0.8614\n",
            "Epoch 4/4\n",
            "60000/60000 [==============================] - 6s 94us/sample - loss: 0.2661 - acc: 0.9256 - val_loss: 0.4429 - val_acc: 0.8638\n",
            "10000/10000 [==============================] - 1s 68us/sample - loss: 0.4429 - acc: 0.8638\n",
            "Test accuracy: 0.8638\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}